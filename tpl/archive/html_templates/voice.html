<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Agent</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            height: 100%;
            overflow: hidden; /* Prevent scrollbars on body */
            background-color: #121212; /* Dark background */
            color: #e0e0e0; /* Light text */
            font-family: sans-serif;
        }
        .container {
            display: flex;
            height: 100%;
            width: 100%;
        }
        .chat-panel {
            width: 25%;
            height: 100%;
            background-color: #1e1e1e; /* Slightly lighter dark */
            padding: 15px;
            box-sizing: border-box;
            display: flex;
            flex-direction: column;
            overflow-y: auto; /* Allow scrolling within chat */
            border-right: 1px solid #333;
        }
        .main-content {
            width: 75%;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 20px;
            box-sizing: border-box;
            overflow: hidden; /* Prevent scrollbars */
        }
        #chat-history {
            flex-grow: 1;
            margin-bottom: 10px;
            overflow-y: auto; /* Scroll within history */
            padding-right: 10px; /* Space for scrollbar */
        }
        #chat-history div {
            margin-bottom: 10px;
            padding: 8px;
            border-radius: 5px;
        }
        .user-message {
            background-color: #333;
            text-align: right;
        }
        .agent-message {
            background-color: #444;
        }
        #chat-input {
            width: calc(100% - 10px);
            padding: 8px;
            border: 1px solid #555;
            background-color: #333;
            color: #e0e0e0;
            border-radius: 4px;
        }
        #voice-meter-container {
            width: 80%;
            height: 100px;
            background-color: #222;
            border: 1px solid #444;
            border-radius: 5px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        #voice-meter {
            width: 0%; /* Starts at 0 width */
            height: 50px;
            background-color: #4CAF50; /* Green */
            transition: width 0.1s ease-out;
        }
        #status {
            margin-top: 20px;
            font-size: 0.9em;
            color: #aaa;
        }
        #record-button {
             margin-top: 20px;
             padding: 10px 20px;
             font-size: 1em;
             cursor: pointer;
             background-color: #555;
             border: none;
             color: #e0e0e0;
             border-radius: 5px;
        }
         #record-button.recording {
             background-color: #d32f2f; /* Red when recording */
         }
    </style>
    <!-- WebLLM Integration -->
    <script type="module">
        import { CreateWebWorkerMLCEngine } from "https://esm.run/@mlc-ai/web-llm";

        const statusElement = document.getElementById("status");
        const chatHistory = document.getElementById("chat-history");
        const chatInput = document.getElementById("chat-input");

        // Use a small model, e.g., "Llama-3-8B-Instruct-q4f32_1-MLC-1k" or "Phi-3-mini-4k-instruct-q4f32_1-MLC"
        // See https://llm.mlc.ai/docs/prebuilt_models.html for options
        const SELECTED_MODEL = "Phi-3-mini-4k-instruct-q4f32_1-MLC";

        let engine;
        let recognition; // SpeechRecognition instance
        let isRecording = false;
        const recordButton = document.getElementById('record-button');
        const voiceMeter = document.getElementById('voice-meter');
        let audioContext;
        let analyser;
        let microphone;
        let javascriptNode;


        async function initializeWebLLM() {
            statusElement.textContent = "Loading WebLLM engine...";
            try {
                engine = await CreateWebWorkerMLCEngine(
                    new Worker(
                        new URL('/static/js/web-llm-worker.js', import.meta.url),
                        { type: 'module' }
                    ),
                    SELECTED_MODEL,
                    {
                        initProgressCallback: (report) => {
                             statusElement.textContent = `Loading model: ${report.text}`;
                             console.log(report);
                        }
                    }
                );
                 statusElement.textContent = "WebLLM Engine and Model Ready.";
            } catch (error) {
                 statusElement.textContent = `Error initializing WebLLM: ${error}`;
                console.error("Error initializing WebLLM:", error);
            }
        }

        function appendMessage(text, sender) {
            const messageDiv = document.createElement("div");
            messageDiv.textContent = text;
            messageDiv.classList.add(sender === 'user' ? 'user-message' : 'agent-message');
            chatHistory.appendChild(messageDiv);
            chatHistory.scrollTop = chatHistory.scrollHeight; // Scroll to bottom
        }

        async function handleChatInput(event) {
            if (event.key === 'Enter' && chatInput.value.trim() !== '') {
                const userInput = chatInput.value.trim();
                appendMessage(userInput, 'user');
                chatInput.value = ''; // Clear input
                await generateResponse(userInput);
            }
        }

        async function generateResponse(prompt) {
             if (!engine) {
                 appendMessage("Error: LLM Engine not ready.", 'agent');
                 return;
             }
             statusElement.textContent = "Agent thinking...";
             try {
                const chunks = await engine.chat.completions.create({
                    stream: true,
                    messages: [{ role: "user", content: prompt }],
                    // Consider adding temperature, max_gen_len etc. if needed
                });

                let reply = "";
                const agentMessageDiv = document.createElement("div");
                agentMessageDiv.classList.add('agent-message');
                chatHistory.appendChild(agentMessageDiv);

                for await (const chunk of chunks) {
                    const deltaContent = chunk.choices[0]?.delta?.content || "";
                    reply += deltaContent;
                    // Update incrementally for streaming effect
                     agentMessageDiv.textContent = reply;
                     chatHistory.scrollTop = chatHistory.scrollHeight;
                }
                statusElement.textContent = "Ready.";
                await engine.runtimeStatsText(); // Optional: log stats

             } catch (error) {
                appendMessage(`Error generating response: ${error}`, 'agent');
                statusElement.textContent = "Error.";
                console.error("LLM Generation Error:", error);
             }
        }

        // --- Voice Recognition & Meter ---

        function setupAudioMeter() {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 256; // Smaller FFT size for volume detection is fine
            javascriptNode = audioContext.createScriptProcessor(2048, 1, 1);

            javascriptNode.onaudioprocess = function() {
                const array = new Uint8Array(analyser.frequencyBinCount);
                analyser.getByteFrequencyData(array);
                let values = 0;
                const length = array.length;
                for (let i = 0; i < length; i++) {
                    values += (array[i]);
                }
                const average = values / length;
                // Scale the average volume to a percentage for the meter width
                const volumePercent = Math.min(100, Math.max(0, average * 2)); // Adjust multiplier as needed
                voiceMeter.style.width = volumePercent + '%';
            }
        }

        function connectMicrophoneToMeter() {
            if (microphone) {
                 microphone.connect(analyser);
                 analyser.connect(javascriptNode);
                 javascriptNode.connect(audioContext.destination);
             }
        }
        function disconnectMicrophoneFromMeter() {
             if (microphone) {
                 javascriptNode.disconnect();
                 analyser.disconnect();
                 microphone.disconnect();
                 voiceMeter.style.width = '0%'; // Reset meter
             }
        }


        function setupSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SpeechRecognition) {
                 statusElement.textContent = "Speech Recognition API not supported in this browser.";
                recordButton.disabled = true;
                return;
            }

            recognition = new SpeechRecognition();
            recognition.continuous = false; // Process after silence
            recognition.lang = 'en-US';
            recognition.interimResults = false; // We only want final results
            recognition.maxAlternatives = 1;

            recognition.onresult = (event) => {
                const transcript = event.results[event.results.length - 1][0].transcript.trim();
                 statusElement.textContent = `Recognized: "${transcript}"`;
                console.log('Voice Result:', transcript);
                if (transcript) {
                    appendMessage(transcript, 'user');
                    generateResponse(transcript); // Send transcribed text to LLM
                }
            };

            recognition.onspeechend = () => {
                // Optionally stop recording automatically when speech ends
                // stopRecording();
                 statusElement.textContent = "Processing speech...";
            };

            recognition.onnomatch = () => {
                 statusElement.textContent = "Speech not recognized.";
            };

            recognition.onerror = (event) => {
                 statusElement.textContent = `Speech recognition error: ${event.error}`;
                console.error("Speech recognition error:", event.error);
                stopRecording(); // Ensure recording stops on error
            };
            
             recognition.onaudiostart = () => {
                statusElement.textContent = "Listening...";
                 connectMicrophoneToMeter();
            };
            recognition.onaudioend = () => {
                 disconnectMicrophoneFromMeter();
                 // statusElement.textContent = "Finished listening."; // Status updated by other events
            };
        }

        async function startRecording() {
            if (isRecording || !recognition) return;
            
             // Request microphone access if not already granted / setup audio context
             if (!audioContext) {
                 setupAudioMeter();
             }
             
            try {
                 // Re-request stream each time to ensure it's active
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                microphone = audioContext.createMediaStreamSource(stream);

                 isRecording = true;
                recordButton.textContent = 'Stop Recording';
                recordButton.classList.add('recording');
                recognition.start();

             } catch (err) {
                 console.error("Error accessing microphone:", err);
                 statusElement.textContent = `Microphone Error: ${err.message}`;
             }
        }

        function stopRecording() {
             if (!isRecording || !recognition) return;
             recognition.stop();
             isRecording = false;
            recordButton.textContent = 'Start Recording';
            recordButton.classList.remove('recording');
             // No need to manually stop the stream tracks here, recognition.stop() handles it
             // Also, stopping tracks would prevent re-use if needed quickly again
             disconnectMicrophoneFromMeter(); // Disconnect meter processing
        }

        recordButton.addEventListener('click', () => {
             if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        });


        // Initialization
        initializeWebLLM();
        setupSpeechRecognition();
        chatInput.addEventListener('keypress', handleChatInput);

    </script>
     <!-- Placeholder for the web worker script -->
     <script src="/static/js/web-llm-worker.js" type="module" defer></script>

</head>
<body>
    <div class="container">
        <div class="chat-panel">
            <h3>Chat</h3>
            <div id="chat-history">
                <!-- Chat messages will appear here -->
                 <div class="agent-message">Hello! How can I help you today?</div>
            </div>
            <input type="text" id="chat-input" placeholder="Type or speak your message...">
        </div>
        <div class="main-content">
            <h2>Voice Input</h2>
            <div id="voice-meter-container">
                 <div id="voice-meter"></div>
            </div>
            <button id="record-button">Start Recording</button>
            <div id="status">Loading...</div>
        </div>
    </div>
</body>
</html>